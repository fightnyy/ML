# 딥러닝 1일차

## 신경망 학습

* 학습: 훈련데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는것
* 손실함수(Loss Function):신경망이 학습할 수 있도록 해주는 지표 -> 이 손실 함수의 결과값을 가장 작게 만드는 가중치 매개변수를 찾는것이 학습의 목표

참고: 선형 분리 가능 문제는 유한번의 학습을 통해 자동으로 학습될수 있지만 비선형 분리문제는 자동으로 학습할 수 없음

> 만약 필기 숫자를 인식할 수 있는 분류기를 만든다고 가정해보자. 이 분류기를 밑바닥에서 부터 짜려면 상당히 피곤할 것이다. 대신 주어진 데이터를 잘 활용하여 특징을 추출하면 좀 더 분류하는데 덜 까다로울 것이다. 단, 여기서 문제에 적합한 특징을 쓰지 않으면(혹은 특징을 설계하지 않으면) 좀 처럼 좋은 결과를 얻을 수 없다.여기서 중요한 점이 사람이 정해줘야 한다는 것이다. 즉 사람이 적절한 특징을 생각해 내야 한다는 것이다. 

하지만 딥러닝은 다르다. 딥러닝은 사람이 개입하지 않는다. 딥러닝은 __종단간 기계학습__이라고 불린다. 처음부터 끝까지 데이터에서 목표한 결과를 사람의 개입없이 얻는다는 의미이다. 

* 신경망의 이점은 모든 문제를 같은 맥락에서 풀 수 있다는 점이다. 사람의 얼굴을 분류하는 문제든, 숫자를 판별하는 문제던.

#### 손실함수

* 신경망은 '하나의 지표'를 기준으로 최적의 매개변수를 기준의로 최적의 매개변수값을 탐색
* 신경망 학습에 사용하는 지표를 __손실함수__ 라고 한다. 손실함수는 임의의 함수를 사용할 수 있지만 일반적으로 __오차제곱합__ 과 __엔트로피 오차__ 를 사용한다.

__주의__ : 손실함수는 신경망의 성능의 __나쁨__ 을 평가하는 지표이다. 따라서 낮으면 낮을 수록 성능이 덜 나쁘다는 것이니까 가장 낮게 만드는 것이 성능을 가장 좋게 만드는 것이다. 



##### 오차제곱합(sse)

$$\frac 1 2 	\sum_{k} (y_k-t_k)^2$$

여기서 y는 예측값 t는 정답값이고 k는 데이터의 차원수를 의미한다. 예를들어 아까 처럼 숫자를 분류하는 분류이기면 k는 10일 것이다.



```
t=[0,0,1,0,0,0,0,0,0,0] #정답은 2

y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]
이럴경우엔 
sse 는 0.0975 이다.

y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]
sse는 0.597 이다.
```

따라서 오차제곱합 기준으로는 첫번째 추정 결과가 정답에 더 가까울 것으로 판단할 수 있다.





##### 교차 엔트로피 오차(cee)

$$
E=-\sum_{k}t_klogy_k
$$

위식은 어려워 보이지만 실질적으로 정답일 때의 추정(tk가 1일 때의 yk)의 자연로그를 계산하는 식이다.

즉, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다.

![img](/Users/nayeong-yun/Desktop/스크린샷 2020-04-27 오후 10.42.20.png)

결과는 위와 같다. 당연히 yk의 값이 1에 가까울수록 값은 0에 가까워 진다. 여기서 delta는 만약 y가 1인경우나 1에 가까울 경우 inf가 나오기 떄문에 해주었다.



### 미니배치 학습

* 기계학습 문제는 훈련데이터에 대한 손실 함수의 값을 구하고, 그값을 최대한 줄여주는 매개변수를 찾아낸다.이렇게 하려면 모든 훈련 데이터를 대상으로 손실함수 값을 구해야 한다. 즉, 훈련 데이터가 100개 있으면 그로부터 계산한 100개의 손실 함수 값들의 합을 지표로 삼아야 한다. 손실 함수의 합을 구하는 방법은 교차 엔트로피 오차로 예를들면
  $$
  E=-\frac 1 N \sum_n \sum_k t_{nk}log y_{nk}
  $$
  로 표현 할 수 있다. 복잡해 보이지만 실질적으로는 다 더하고 n 개로 나눈 것만 다른것이다. 이렇게 되면 데이터셋이 10000개든 100000개든 구할 수 있다. 하지만 실제로 구하려면 상당히 오랜시간이 걸릴 것이다. 그래서 데이터 일부를 추려 __근사치__  로 구할 수 있다. 훈련데이터로 부터 일부만 골라 학습을 수행할 수 있다. 여기서 그 일부를 __미니배치__ 라고 한다. 이런 학습방법을 __미니배치 학습__ 이라고 한다. 만약 데이터셋에서 10장만 빼내려고 하면 어떻게 해야할까? 

  이때는 __np.random.choice()__ 함수를 쓰면 해결 할 수 있다.

  ![choice funtion](/Users/nayeong-yun/Desktop/스크린샷 2020-04-27 오후 10.58.44.png)

여기서 one_hot_label을 하여 원 핫 인코딩을 한 것을 알 수 있다. 우선 np.random.choice함수는 첫번째파라미터로 0이상 60000미만의 수중 무작위로 10개를 골라낸것을 의미한다.

손실함수도 미니배치로 계산한다.
$$f(x)= if x < x_{min} : (x/x_{min})^a$$
